{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from kobert import get_tokenizer\n",
        "from kobert import get_pytorch_kobert_model\n",
        "import gluonnlp as nlp\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# In[27]:\n",
        "\n",
        "\n",
        "class data2txt():\n",
        "    def pd2txt(path):\n",
        "        tsv = pd.read_csv(path).drop('Unnamed: 0',axis=1)\n",
        "        path = './temp.txt'\n",
        "        with open(path,'w',encoding='UTF-8') as f:\n",
        "            for name in list(tsv['\uad00\uad11\uc9c0\uc18c\uac1c']):\n",
        "                f.write(name+'\\n')\n",
        "        print('pd2txt done')\n",
        "        return path\n",
        "    def word2txt(word_list):\n",
        "        path = './temp.txt'\n",
        "        with open(path,'w',encoding='UTF-8') as f:\n",
        "            for name in word_list:\n",
        "                f.write(name+'\\n')\n",
        "        print('word2txt done')\n",
        "        return path\n",
        "\n",
        "\n",
        "# In[28]:\n",
        "\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.sentences[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.dataset))\n",
        "\n",
        "\n",
        "# In[29]:\n",
        "\n",
        "\n",
        "class Custom_Bert():\n",
        "    def __init__(self,df_path = None,word_list = None):\n",
        "        self.word_list = word_list\n",
        "        self.df_path = df_path\n",
        "        \n",
        "    def config(self,txt_path):\n",
        "        self.model, vocab = get_pytorch_kobert_model(cachedir=\".cache\")\n",
        "        tokenizer = get_tokenizer()\n",
        "        tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "        tsv_txt = nlp.data.TSVDataset(txt_path,field_indices=[0])\n",
        "        return tsv_txt,tok\n",
        "    \n",
        "    def BertDL(self,tsv,sent_idx,bert_tokenizer,max_len,pad,pair):\n",
        "        dataset = BERTDataset(tsv,sent_idx,bert_tokenizer,max_len,pad,pair)\n",
        "        DL = torch.utils.data.DataLoader(dataset,batch_size = 1)\n",
        "\n",
        "        return DL\n",
        "    \n",
        "    def gen_attention_mask(self,token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "    \n",
        "    def get_vector(self,DL):\n",
        "        device = torch.device(\"cpu\")\n",
        "        embed_layer = torch.empty([1,768])\n",
        "        for input_ids,valid_length,token_type_ids  in tqdm(DL):\n",
        "            input_ids = input_ids.long().to(device)\n",
        "            input_mask = self.gen_attention_mask(input_ids,valid_length).long().to(device)\n",
        "            _,pooled_output = self.model(input_ids,input_mask,token_type_ids)\n",
        "            embed_layer = torch.cat([embed_layer,pooled_output])\n",
        "        return embed_layer\n",
        "    \n",
        "    def get_df_vector(self):\n",
        "        path = data2txt.pd2txt(self.df_path)\n",
        "        tsv,tok = self.config(path)\n",
        "        DL = self.BertDL(tsv,0,tok,15,True,False)\n",
        "        embed_layer = self.get_vector(DL)\n",
        "        return embed_layer\n",
        "    \n",
        "    def get_word_vector(self):\n",
        "        path = data2txt.word2txt(self.word_list)\n",
        "        tsv,tok = self.config(path)\n",
        "        DL = self.BertDL(tsv,0,tok,15,True,False)\n",
        "        embed_layer = self.get_vector(DL)\n",
        "        return embed_layer\n",
        "\n",
        "\n",
        "# In[30]:\n",
        "\n",
        "\n",
        "# word = ['\ud734\uc2dd','\uccb4\ud5d8','\ubb38\ud654','\ubcf5\ud569']\n",
        "# bert_config = Custom_Bert(word_list = word)\n",
        "# bert_config = Custom_Bert(df_path = '../tourismData.csv')\n",
        "\n",
        "\n",
        "# In[31]:\n",
        "\n",
        "\n",
        "# embed_layer = bert_config.get_df_vector()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}